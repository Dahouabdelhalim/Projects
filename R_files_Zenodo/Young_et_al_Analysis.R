#-----------------------------------------------------#
# Code accompanying Young et al. Long-term climate and competition explain forest mortality patterns under extreme drought
# Code file 2 of 2: Analysis
# The code in this file performs statistical analyses and plots results
# This file relies on a data.frame (generated by code file 1) in which each data row represents an aerially-surveyed 3.5-km grid cell in a specific year
# The data file (with metadata added) is also in this Dryad data repository and is named "Young_et_al_Data.xlsx". To load it into this script, save the worksheet containing the data (as opposed to the metadata) as a CSV file.
# Authors of this script: Derek Young, Jens Stevens, J. Mason Earles, Andrew Latimer
#-----------------------------------------------------#

#-----------------------------------------------------#
####  1. Load Libraries and Functions  ################
#-----------------------------------------------------#

# Specify necessary packages
p <- c("gtable","dplyr","arm","ggplot2","reshape2","gridExtra","plyr","scales","stringr","viridis","spatial","sp","raster","rgdal",
        "rasterVis","pROC","geoR","car","rgeos","maptools","spBayes","gridExtra","gstat","nlme","mgcv","Cairo","ggsn","rstan","mvtnorm","gstat")

# If necessary: install/update packages
# install.packages(p)

# Load packages
lapply(p,require,character.only=TRUE)


### Define functions ###

## Inverse logit function ##
invlogit <- function(x) {exp(x)/(1+exp(x))}

## Rasterize function ##
# Convert a data.frame with coordinates to SpatialPoints and then to raster
# The resulting raster will not have a projection; one will need to be assigned after rasterization based on the projection of the coordinates in the data.frame
# PARAMETERS:
#   sp.d: data.frame with coordinates in columns "alb.x" and "alb.y"
#   layer: the column of the data.frame from which values are to be taken to produce the raster
am_rasterize=function(sp.d,layer){
  coordinates(sp.d)= ~alb.x + alb.y
  gridded(sp.d)=T
  r=raster(sp.d,layer=layer)
  return(r)
}

## Model fitting function ##
# Take a data.frame with predictor and response variables and fit a hurdle model (Gaussian and binomial components)
# Fit hurdle model in two ways: spatial regression using package spBayes, and non-spatial regression
# PARAMETERS
#   data: data.frame representing one year of aerial survey data (each row representing one 3.5-km raster cell); with spatial coordinates, predictor variables (centered an standardized), and response variables named as specified in the models within this function
#   knots.bin: locations of knots for the binomial spatial model
#   knots.tph: locations of knots for the Gaussian spatial model
spBayesFit <- function(data,knots.bin,knots.tph) {
  
  data.mort <- data[data$mort.bin == TRUE,] # subset of the data with only those raster cells where mortality was observed; for Gaussian (mortalty amount) model
  
  # Fit non-spatial models
  m1.tph = lm(log(mort.tph)~Defnorm_std*live.bah_std+ I(Defnorm_std^2)+I(live.bah_std^2)+ Defz0, data=data.mort)
  m1.bin = glm(mort.bin~Defnorm_std*live.bah_std+I(live.bah_std^2)+ Defz0, data=data, family="binomial")
  
  # Preparing to run Gaussian spatial model; get lognormal model phi (spatial decay parameter) estimate
  spdata <- data.mort
  z = resid(lm(log(mort.tph)~Defnorm_std*live.bah_std+ I(Defnorm_std^2)+I(live.bah_std^2)+ Defz0, spdata)) # residuals of non-spatial model
  spdata$z = z
  coordinates(spdata) = spdata[,c("alb.x", "alb.y")]
  test.vgm = variogram(z~1, data=spdata)
  test.fit = fit.variogram(test.vgm, model = vgm(1, "Exp", 200, 10))
  tph.phi <- test.fit[2,]$range # extract phi value from foted variogram
  if(data$year[1]==2009) { # for 2009, variogram did not converge, so
    tph.phi <- 152.6631 # use value from 2012, the year with greatest phi
  }
  
  # Preparing to run binomial spatial model; get binomial model phi (spatial decay parameter) estimate
  spdata = data
  z = resid(glm(mort.bin~Defnorm_std*live.bah_std+I(live.bah_std^2)+ Defz0, spdata, family="binomial"))
  spdata$z = z
  coordinates(spdata) = spdata[,c("alb.x", "alb.y")]
  test.vgm = variogram(z~1, data=spdata)
  test.fit = fit.variogram(test.vgm, model = vgm(1, "Exp", 200, 10))
  test.fit
  bin.phi <- test.fit[2,]$range # extract phi value from foted variogram
  
  ## Fit spBayes Gaussian model using phi estimate from above
  spdata = data.mort
  coords=as.matrix(spdata[,c("alb.x", "alb.y")])
  
  # Set priors: loose priors on beta and residual error variance (tausq), and on spatial variance parameter (sigma.sq), but very tight on Phi (spatial decay parameter). 
  priors = list("beta.Norm"=list(rep(0,7), diag(100,7)) , "phi.Unif" = c(1/tph.phi-.001, 1/tph.phi+.001), "sigma.sq.IG" = c(5,5), tau.sq.IG = c(2,0.1))
  
  # Set starting and tuning values
  starting = list("phi"=1/tph.phi, "sigma.sq"=1+1, "tau.sq"=1+0.2)
  tuning = list("phi"=0.01, "sigma.sq"=0.1, "tau.sq"=0.1)
  
  # Run Gaussian spatial model
  tph.splm = spLM(log(mort.tph)~Defnorm_std*live.bah_std + I(Defnorm_std^2)+I(live.bah_std^2)+ Defz0, data=spdata, coords=coords, knots=knots.tph, cov.model = "exponential", priors=priors, tuning=tuning, starting = starting, n.samples=10000)
  
  # Recover parameter samples, ignoring the first half of the run as burn-in. 
  tph.splm = spRecover(tph.splm, get.beta=TRUE, get.w=TRUE, start=5001)
  beta.hat.tph <- tph.splm$p.beta.recover.samples[,1:7]
  theta.hat.tph <- tph.splm$p.theta.recover.samples
  beta.hat.tph <- as.data.frame(beta.hat.tph)
  theta.hat.tph <- as.data.frame(theta.hat.tph) 
  
  ## Fit spBayes binomial model using phi estimate from above
  spdata = data
  coords=as.matrix(spdata[,c("alb.x", "alb.y")])
  
  # Set priors: loose priors on beta and residual error variance (tausq), and on spatial variance parameter (sigma.sq), but very tight on Phi (spatial decay parameter). 
  priors = list("beta.Norm"=list(rep(0,2), diag(100,2)) , "phi.Unif" = c(1/bin.phi-.001, 1/bin.phi+.001), "sigma.sq.IG" = c(5,5), tau.sq.IG = c(2,0.1))
  
  # Set starting and tuning values
  fit = glm(mort.bin~Defnorm_std*live.bah_std + I(live.bah_std^2)+ Defz0, data=spdata, family="binomial")
  starting = list("phi"=1/bin.phi, "sigma.sq"=1, "tau.sq"=1, "beta" = coef(fit), w=rnorm(nrow(knots.bin), 0, 0.1))
  tuning=list("phi"=0.01, "sigma.sq"=0.1, "tau.sq"=0.1, "beta"=t(chol(vcov(fit))), w=0.1)
  
  
  # Run binomial spatial model
  bin.spglm = spGLM(mort.bin~Defnorm_std*live.bah_std + I(live.bah_std^2)+ Defz0, data=spdata, family="binomial", coords=coords, knots=knots.bin, cov.model = "exponential", priors=priors, tuning=tuning, starting = starting, amcmc =list(batch.length=50, n.batch=200, accept.rate=0.3), verbose=TRUE, n.report=10)
  
  # Recover parameter samples, ignoring the first half of the run as burn-in. 
  sub.samps=5001:10000
  beta.hat.bin <- bin.spglm$p.beta.theta.samples[sub.samps,1:7]
  
  #return parameter samples (from spatial and non-spatial Gaussian and binomial models), as well as fitted spatial model objects
  ret <- list(beta.hat.bin=beta.hat.bin,beta.hat.tph=beta.hat.tph,theta.hat.tph=theta.hat.tph,tph.splm=tph.splm,bin.spglm=bin.spglm)
  return(ret)
  
}




#-----------------------------------------------------#
####  2. Load and process data  #######################
#-----------------------------------------------------#

##Load original data file:

d = read.csv("Young_et_al_Data.csv") # In this dataset, each row represents a specific raster cell in a specific year. See metadata associated with this data file on Dryad

##For purposes of model fitting, change the x/y coordinates from m to km:
d$alb.x = d$alb.x/1000
d$alb.y = d$alb.y/1000

d$year <- as.character(d$year)

d$Defz012 <- apply(d[,c("Defz0","Defz1","Defz2")],1,mean) # Compute a 3-year average deficit z-score




#-------------------------------------------------------------#
####  3. Center and standardize the explanatory variables  ####
#-------------------------------------------------------------#

d.unique=d[which(!duplicated(d$alb.x*d$alb.y)),] #only count each cell once, even if it was surveyed in multiple years
cols.to.stdize = c("live.bah", "Defnorm")
scale.attr = list()
for (i in 1:length(cols.to.stdize)) {
  d.unique [,paste0(cols.to.stdize[i],"_std")] = scale(d.unique[,cols.to.stdize[i]]) #Scale only unique dataset
  scale.attr[[i]] = attributes(scale(d.unique[,cols.to.stdize[i]])) #record scale and center attributes for plotting later
  #Assign scaled values from unique dataset to full dataset: find the index of d.unique with the same value of cols.to.stdize, and assign its standardized value to the full dataset
  d[,paste0(cols.to.stdize[i], "_std")]=d.unique[
    pmatch(x=(d$alb.x*d$alb.y),table=(d.unique$alb.x*d.unique$alb.y),duplicates.ok = TRUE), #row: match coords in d to their unique counterpart in d.unique
    paste0(cols.to.stdize[i], "_std") #column: change name to reflect that it was standardized
    ] 
} #End loop standardizing each variable specified
names(scale.attr) <- cols.to.stdize #rename scale and center attributes for each parameter





#-----------------------------------------------------#
####  4. Model selection  #############################
#-----------------------------------------------------#

# Compare models based on 2015 data
d.2015 = d[d$year==2015 ,]
d.2015.mort = d.2015[d.2015$mort.bin==1 ,] # subset to include only grid cells with mortality


# Fit candidate binomial (mortality probability) models
m.bin.1=glm(mort.bin~Defnorm_std + Defz0, data=d.2015, family="binomial")
m.bin.2=glm(mort.bin~Defnorm_std + I(Defnorm_std^2) + Defz0, data=d.2015, family="binomial")
m.bin.3=glm(mort.bin~Defnorm_std + live.bah_std + Defz0, data=d.2015, family="binomial")
m.bin.4=glm(mort.bin~Defnorm_std + I(Defnorm_std^2) + live.bah_std + Defz0, data=d.2015, family="binomial")
m.bin.5=glm(mort.bin~Defnorm_std + I(live.bah_std^2) + live.bah_std + Defz0, data=d.2015, family="binomial")
m.bin.6=glm(mort.bin~Defnorm_std + I(live.bah_std^2) + I(Defnorm_std^2) + live.bah_std + Defz0, data=d.2015, family="binomial")
m.bin.7=glm(mort.bin~Defnorm_std * live.bah_std + Defz0, data=d.2015, family="binomial")
m.bin.8=glm(mort.bin~Defnorm_std * live.bah_std + I(Defnorm_std^2) + Defz0, data=d.2015, family="binomial")
m.bin.9=glm(mort.bin~Defnorm_std * live.bah_std + I(live.bah_std^2) + Defz0, data=d.2015, family="binomial")
m.bin.10=glm(mort.bin~Defnorm_std * live.bah_std + I(live.bah_std^2) + Defz012, data=d.2015, family="binomial")
m.bin.11=glm(mort.bin~Defnorm_std * live.bah_std + I(live.bah_std^2) + I(Defnorm_std^2) + Defz0, data=d.2015, family="binomial")
m.bin.12=glm(mort.bin~Defnorm_std * live.bah_std + I(live.bah_std^2) + I(Defnorm_std^2) + Defz012, data=d.2015, family="binomial")

# Print BIC table
bics <- BIC(m.bin.1,m.bin.2,m.bin.3,m.bin.4,m.bin.5,m.bin.6,m.bin.7,m.bin.8,m.bin.9,m.bin.10,m.bin.11,m.bin.12)
bics$d.bic <- bics$BIC-min(bics$BIC)
bics

# Get predicted and observed mortality values for best 2015 model and compute AUC
predicted <- predict(m.bin.9)
observed <- d.2015$mort.bin
auc(observed,predicted)


# Fit candidate Gaussian (mortality amount) models
m.tph.1=lm(log(mort.tph)~Defnorm_std + Defz0, data=d.2015.mort)
m.tph.2=lm(log(mort.tph)~Defnorm_std + I(Defnorm_std^2) + Defz0, data=d.2015.mort)
m.tph.3=lm(log(mort.tph)~Defnorm_std + live.bah_std + Defz0, data=d.2015.mort)
m.tph.4=lm(log(mort.tph)~Defnorm_std + I(Defnorm_std^2) + live.bah_std + Defz0, data=d.2015.mort)
m.tph.5=lm(log(mort.tph)~Defnorm_std + I(live.bah_std^2) + live.bah_std + Defz0, data=d.2015.mort)
m.tph.6=lm(log(mort.tph)~Defnorm_std + I(live.bah_std^2) + I(Defnorm_std^2) + live.bah_std + Defz0, data=d.2015.mort)
m.tph.7=lm(log(mort.tph)~Defnorm_std * live.bah_std + Defz0, data=d.2015.mort)
m.tph.8=lm(log(mort.tph)~Defnorm_std * live.bah_std + I(Defnorm_std^2) + Defz0, data=d.2015.mort)
m.tph.9=lm(log(mort.tph)~Defnorm_std * live.bah_std + I(live.bah_std^2) + Defz0, data=d.2015.mort)
m.tph.10=lm(log(mort.tph)~Defnorm_std * live.bah_std + I(live.bah_std^2) + Defz012, data=d.2015.mort)
m.tph.11=lm(log(mort.tph)~Defnorm_std * live.bah_std + I(live.bah_std^2) + I(Defnorm_std^2) + Defz0, data=d.2015.mort)
m.tph.12=lm(log(mort.tph)~Defnorm_std * live.bah_std + I(live.bah_std^2) + I(Defnorm_std^2) + Defz012, data=d.2015.mort)

# Print BIC table
bics <- BIC(m.tph.1,m.tph.2,m.tph.3,m.tph.4,m.tph.5,m.tph.6,m.tph.7,m.tph.8,m.tph.9,m.tph.10,m.tph.11,m.tph.12)
bics$d.bic <- bics$BIC-min(bics$BIC)
bics

# Get R-sq value of best model
summary(m.tph.11)




#--------------------------------------------------------#
####  5. Explore and plot collinearity of predictors  ####
#--------------------------------------------------------#

# Create data frame of predictor variables
coll.df <- d.2015[,c("Defnorm_std","live.bah_std","Defz0")]
names(coll.df) <- c("CWD_normal","basal_area","CWD_zscore")
coll.df$"CWD_normalXbasal_area" <- coll.df$CWD_normal * coll.df$basal_area
coll.df$"CWD_normal_sq" <- coll.df$CWD_normal^2
coll.df$"basal_area_sq" <- coll.df$basal_area^2


# Define a function to use for printing the correlation coefficient of a given pair of predictors
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  r <- round(r,2)
  txt <- format(c(r, 0.123456789), digits = 2)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor/2)
}

# Produce a pairs plot
pairs(coll.df,upper.panel=panel.cor,cex.labels=1.2)





#-------------------------------------------------------------------------#
####  6. Fit spatial and non-spatial statistical models for all years  ####
#-------------------------------------------------------------------------#

## For spatial models, identify the locations to use for knots (for all years)

# Knots for mortality presence/absence (logistic) models (uses all grid cells flown)
coords=as.matrix(d[,c("alb.x", "alb.y")]) #First get all coordinates used, but only once each
coords.unique <- unique(coords)
knots.bin = kmeans(coords.unique, 200,iter.max=100)$centers # use k-means clustering to identify regions (and their centers)

# Knots for mortality amount (Gaussian) models (uses all flow grid cells that had any mortality)
coords=as.matrix(d[d$mort.tph>0,c("alb.x", "alb.y")]) #First get all coordinates used, but only once each
coords.unique <- unique(coords)
knots.tph = kmeans(coords.unique, 200,iter.max=100)$centers

##Initialize lists to hold model output from different years before running across all years

m.tph.sim.coef <- m.tph.sim.sigma <- m.bin.sim.coef <- list() # for holding tph (amount) and bin (presence/absence) model coefficients from spatial models
m.full.tph.sim.coef <- m.full.tph.sim.sigma <- m.full.bin.sim.coef <- list() # for holding tph (amount) and bin (presence/absence) model coefficients from non-spatial models ("full" actually means nonspatial)
tph.splm <- bin.spglm <- list() # for holding fitted spatial model objects

##Run for each year
year.opts <- c(2009:2015)

for (year in year.opts){

  ### 6a. spatial model using spBayes ###
  ##subset the dataset to the focal year
  d.year <- d[d$year==year,]
  
  ## Fit the Gaussian and logistic spatial models
  param.samples <- spBayesFit(d.year,knots.bin,knots.tph) # function defined near top of this script
  
  ## Compile the sampled coefficients from all models into lists (one list element for each year)
  m.tph.sim.coef[[as.character(year)]] <- as.data.frame(param.samples["beta.hat.tph"]) #Gaussian coefficients
  m.tph.sim.sigma[[as.character(year)]] <- sqrt(as.data.frame(param.samples["theta.hat.tph"])$theta.hat.tph.tau.sq) # sigma
  m.bin.sim.coef[[as.character(year)]] <- as.data.frame(param.samples["beta.hat.bin"]) #logistic coefficients
  tph.splm[[as.character(year)]] <- param.samples["tph.splm"] #the actual spatial model object itself (Gaussian)
  bin.spglm[[as.character(year)]] <- param.samples["bin.spglm"] #the actual spatial model object itself (logistic)

  ### 6b. nonspatial model ###

  # Fit Gaussian model
  m.full.tph=
    lm(log(mort.tph)~Defnorm_std*live.bah_std + 
         I(Defnorm_std^2) + I(live.bah_std^2) + Defz0, 
       data = d.year[d.year$mort.bin==TRUE,])
    
  # Fit logistic model
  m.full.bin=
    glm(mort.bin ~ Defnorm_std*live.bah_std + 
          I(live.bah_std^2) + Defz0, 
        data = d.year,family="binomial")
  
  # Optionally compute and display variance inflation factors (VIFs)
  #   vif(m.full.tph)
  #   vif(m.full.bin)
  

  ##Store the coefficients from each model (Gaussian and logistic) (full, nonspatial), simulated 10000 times.
  m.full.tph.sim = sim(m.full.tph,n.sims=10000)
  m.full.tph.sim.coef[[as.character(year)]] = as.data.frame(coef(m.full.tph.sim)) # coefficients
  m.full.tph.sim.sigma[[as.character(year)]] <- m.full.tph.sim@sigma # model sigma
  
  m.full.bin.sim = sim(m.full.bin,n.sims=10000)
  m.full.bin.sim.coef[[as.character(year)]] = as.data.frame(coef(m.full.bin.sim))
  m.full.bin.sim.coef[[as.character(year)]]$sigma.sq <- 1 # unused placeholder; for consistency with Gaussian model

  
  ##Change the names of the spBayes model coefficients (set them equal to the non-spatial model coefficient names) -- NOTE: must specify spatial and non-spatial models in the same order for this to work correctly
  names(m.tph.sim.coef[[as.character(year)]]) <- names(m.full.tph.sim.coef[[as.character(year)]])
  names(m.bin.sim.coef[[as.character(year)]]) <- names(m.full.bin.sim.coef[[as.character(year)]])
  
  
}


## Store the non-spatial model coefficients into the same list as the spatial model coefficients, indexed, e.g., as: "2015_nonspatial" (whereas the spatial model would be indexed simply "2015")
## Additionally store the data used to fit the spatial and non-spatial models for each year into a single data frame (with one row per grid cell, per year, per model type [spatial or non])
for(year in year.opts) {
  
  year.nonspatial <- paste0(year,"_nonspatial")
  
  m.tph.sim.coef[[year.nonspatial]] <- m.full.tph.sim.coef[[year]]
  m.bin.sim.coef[[year.nonspatial]] <- m.full.bin.sim.coef[[year]]
  m.tph.sim.sigma[[year.nonspatial]] <- m.full.tph.sim.sigma[[year]]
                                 
  d.nonspatial <- d[d$year==year,]
  d.nonspatial$year <- year.nonspatial
  d <- rbind(d,d.nonspatial)
  
}


## check convergence

##tph model
for(year in 2009:2015) {
  samples <- tph.splm[[as.character(year)]][["tph.splm"]]
  #plot(samples$p.beta.recover.samples)
  plot(samples$p.theta.recover.samples)
}

##mortbin model
for(year in 2009:2015) {
  samples <- bin.spglm[[as.character(year)]][["bin.spglm"]]
  plot(samples$p.beta.theta.samples)
}

# Optionally, save the model-estimated parameter samples and fitted model objects
# save.image("fitmodels.RData")

#-------------------------------------------------------------------------------------------------------#
####  7. Optionally, load model coefficients and other large analytical output, if previously saved  ####
#-------------------------------------------------------------------------------------------------------#
# If model fitting was previously run, can start script here by loading image (next line) so you don't have to run all the spBayes models
# load("fitmodels.RData")




#-------------------------------------------------------#
####    8. Save a file with coefficient summaries    ####
#-------------------------------------------------------#

conflim <- c(0.025,0.975) # limits to capture central 95% confidence/credible interval

bin.coef.df <- data.frame() # for storing binomial model coefficients
tph.coef.df <- data.frame() # for storing Gaussian model coefficients
bin.coef.num.df <- data.frame() # for storing binomial model coefficients including upper and lower confidence/credible bounds
tph.coef.num.df <- data.frame()  # for storing Gaussian model coefficients including upper and lower confidence/credible bounds

years.opts <- names(m.tph.sim.coef)

## This loop populates tph.coef.num.df and bin.coef.num.df with coefficients and 95% intervals for each coefficient and each year
for (year in years.opts){
 bin.sim.coef <- m.bin.sim.coef[[as.character(year)]]
 bin.median <- apply(bin.sim.coef,2,median)
 bin.median <- t(as.data.frame(bin.median))
 bin.median <- signif(bin.median,3)
 bin.median <- formatC(bin.median,digits=3,format="fg",flag="#")
 bin.confint <- apply(bin.sim.coef,2,quantile,probs=conflim)
 bin.lwr <- bin.confint[1,]
 bin.upr <- bin.confint[2,]
 
 bin.lwr <- signif(bin.lwr,3)
 bin.upr <- signif(bin.upr,3)
 bin.lwr <- formatC(bin.lwr,digits=3,format="fg",flag="#")
 bin.upr <- formatC(bin.upr,digits=3,format="fg",flag="#")
 bin.bounds <- paste("(",bin.lwr,",",bin.upr,")",sep="")
 bin.full <- paste(bin.median,bin.bounds)
 names(bin.full) <- colnames(bin.sim.coef)
 bin.full <- as.data.frame(t(bin.full))
 bin.full$year <- year
 bin.coef.df <- rbind(bin.coef.df,bin.full)
 
 bin.full.num <- as.data.frame(rbind(bin.median,bin.lwr,bin.upr))
 bin.full.num$val <- c("median","lower","upper")
 bin.full.num$year <- year
 bin.coef.num.df <- rbind(bin.coef.num.df,bin.full.num)
 
 
 tph.sim.coef <- m.tph.sim.coef[[as.character(year)]]
 tph.median <- apply(tph.sim.coef,2,median)
 tph.median <- t(as.data.frame(tph.median))
 tph.median <- signif(tph.median,3)
 tph.median <- formatC(tph.median,digits=3,format="fg",flag="#")
 tph.confint <- apply(tph.sim.coef,2,quantile,probs=conflim)
 tph.lwr <- tph.confint[1,]
 tph.upr <- tph.confint[2,]
 tph.lwr <- signif(tph.lwr,3)
 tph.lwr <- formatC(tph.lwr,digits=3,format="fg",flag="#")
 tph.upr <- signif(tph.upr,3)
 tph.upr <- formatC(tph.upr,digits=3,format="fg",flag="#")
 tph.bounds <- paste("(",tph.lwr,",",tph.upr,")",sep="")
 tph.full <- paste(tph.median,tph.bounds)
 names(tph.full) <- colnames(tph.sim.coef)
 tph.full <- as.data.frame(t(tph.full))
 tph.full$year <- year
 tph.coef.df <- rbind(tph.coef.df,tph.full)
 
 tph.full.num <- as.data.frame(rbind(tph.median,tph.lwr,tph.upr))
 tph.full.num$val <- c("median","lower","upper")
 tph.full.num$year <- year
 tph.coef.num.df <- rbind(tph.coef.num.df,tph.full.num)
 
}

## Optionally, write files with coefficient summaries
# write.csv(bin.coef.df,"binomial_coefs.csv",row.names=FALSE)
# write.csv(tph.coef.df,"tph_coefs.csv",row.names=FALSE)




#-----------------------------------------------------#
####  9. Model predictions  ###########################
#-----------------------------------------------------#

## Define one data frame of predictor variables with variation along Defnorm
# define the values of "live.bah" that are interesting for prediction plots
live.bah.med <- c(20) # "intermediate" basal area
# define the values of "Defnorm" that are interesting for prediction plots
Defnorm.range <- seq(400,1000,by=10) #set range of Defnorm_std values
# define a set of the predictor values for which to simulate responses
newdat1 <- as.data.frame(expand.grid(live.bah.med,Defnorm.range))

## Define another data frame of predictor variables (with variation along live.bah, for two levels of Defnorm)
# define the values of "live.bah" that are interesting for prediction plots
live.bah.med <- seq(from=0,to=50,by=0.5)
# define the values of defnorm that are interesting for prediction plots
Defnorm.range <- c(600,900)
# define a set of the predictor values for which to simulate responses
newdat2 <- as.data.frame(expand.grid(live.bah.med,Defnorm.range))

newdat <- rbind(newdat1,newdat2)
names(newdat) <- c("live.bah","Defnorm")

##create standardized versions of these predictors
newdat$Defnorm_std <- (newdat$Defnorm - scale.attr$Defnorm$'scaled:center') / scale.attr$Defnorm$'scaled:scale'
newdat$live.bah_std <- (newdat$live.bah - scale.attr$live.bah$'scaled:center') / scale.attr$live.bah$'scaled:scale'

##define limits for confidence and prediction intervals
conflim <- c(0.025,0.975)


years.opts <- names(m.tph.sim.coef) #the years to make predictions for
counterfactual.df.list <- list() #to hold the predictor variables and simulated responses (one list element for each year). "counterfactual" refers to the fact that we are making predictions for data points that do not exactly replicate the observed data points

#Generate predictions for each year, for each different combination of predictor variable values
for(year in years.opts) {
  # set data to year in question
  d.sub = d[which(d$year==year),]

  # select the simulated parameter values that correspond to the current year
  bin.param.sim.mat <- m.bin.sim.coef[[as.character(year)]]
  tph.param.sim.mat <- m.tph.sim.coef[[as.character(year)]]
  tph.sigma.sim.vect <- m.tph.sim.sigma[[as.character(year)]]
  
  # get the median Defz0 value for the year (to use for all predictions)
  newdat$Defz0 <- median(d.sub$Defz0)
  
  # to store model predictions (and predictor variables used to obtain those predictions)
  counterfactual.df <- data.frame() # "counterfactual" refers to the fact that we are making predictions for data points that do not exactly replicate the observed data points
  
  for(i in 1:nrow(newdat)) { #for each set of predictor variable values to use for predictions, predict the response (predict it many times and then summarize those predictions to get 95% intervals)
    
    ## Binomial part
    
    names(bin.param.sim.mat)[5] <- "Defz0"
    
    # calculate mortality probability (in logit-link space)
    bin.response <- bin.param.sim.mat[,"(Intercept)"] +
      bin.param.sim.mat[,"Defnorm_std"] * newdat[i,"Defnorm_std"] +
      bin.param.sim.mat[,"live.bah_std"] * newdat[i,"live.bah_std"] +
      bin.param.sim.mat[,"I(live.bah_std^2)"] * newdat[i,"live.bah_std"]^2 +
      bin.param.sim.mat[,"Defz0"] * newdat[i,"Defz0"] +
      bin.param.sim.mat[,"Defnorm_std:live.bah_std"] * newdat[i,"Defnorm_std"] * newdat[i,"live.bah_std"]
    
    # transform back to response scale
    bin.response.logit <- invlogit(bin.response) 
    
    # calculate response confidence/credible interval and median response based on simulated responses
    bin.response.confint <- as.data.frame(t(quantile(bin.response.logit,probs=conflim))) 
    names(bin.response.confint) <- c("bin.confint.low","bin.confint.high")
    bin.response.median <- median(bin.response.logit)
    
    # make a random prediction (0 or 1) based on the simulated mortality probabilty
    bin.response.random <- rbinom(n=length(bin.response.logit),prob=bin.response.logit,size=1) 
    
    
    ## Mortality extent (lognormal) part
    
    # calculate mean mortality extent (in log-transformed scale)
    tph.response <- tph.param.sim.mat[,"(Intercept)"] +
      tph.param.sim.mat[,"Defnorm_std"] * newdat[i,"Defnorm_std"] +
      tph.param.sim.mat[,"live.bah_std"] * newdat[i,"live.bah_std"] +
      tph.param.sim.mat[,"I(Defnorm_std^2)"] * newdat[i,"Defnorm_std"]^2 +
      tph.param.sim.mat[,"I(live.bah_std^2)"] * newdat[i,"live.bah_std"]^2 +
      tph.param.sim.mat[,"Defz0"] * newdat[i,"Defz0"] +
      tph.param.sim.mat[,"Defnorm_std:live.bah_std"] * newdat[i,"Defnorm_std"] * newdat[i,"live.bah_std"]
    
    # convert back to response scale
    tph.response.exp <- exp(tph.response) 
    
    # calculate response confidence/credible interval and median response based on simulated responses
    tph.response.confint <- as.data.frame(t(quantile(tph.response.exp,probs=conflim))) 
    names(tph.response.confint) <- c("tph.confint.low","tph.confint.high")
    tph.response.median <- median(tph.response.exp)
    
    # draw random predictions of the response variable based on the simulated mean mort.tph and SD
    tph.response.random <- exp(rnorm(n=length(tph.response),mean=tph.response,sd=tph.sigma.sim.vect)) 
    
    ## combine mortality extent and mortality probability to make overall ("conditional") predictions and confidence/credible interval
    cond.response <- bin.response.logit * tph.response.exp
    cond.response.median <- median(cond.response,na.rm=T)
    cond.response.confint <- as.data.frame(t(quantile(cond.response,probs=conflim)))
    names(cond.response.confint) <- c("cond.confint.low","cond.confint.high")
    cond.response.random <- bin.response.random * tph.response.random

    ##compile all prediction summaries (and the predictor values associated with them) into a data frame
    # "counterfactual" refers to the fact that we are making predictions for data points that do not exactly replicate the observed data points
    counterfactual.df.row = data.frame(bin.response.median,
                                        bin.response.confint,
                                        tph.response.median,
                                        tph.response.confint,
                                        cond.response.median,
                                        cond.response.confint,
                                        newdat[i,])
    
    counterfactual.df <- rbind(counterfactual.df,counterfactual.df.row) # add the values for this set of predictor variable values onto the data frame holding all predictions

  }
  
  counterfactual.df.list[[as.character(year)]] <- counterfactual.df # add the data frame of predictor variables and predictions for this year onto the list holding all years

}





#--------------------------------------------------------------#
####   10. Fig. 1- Maps of CWD and basal area region-wide   ####
#--------------------------------------------------------------#

d.unique=d[which(!duplicated(d$alb.x*d$alb.y)),] # only show each grid cell once (the full data frame can have multiple years per grid cell)

## Generate California boundary
CA = readOGR(dsn="./GIS", layer="CA_Boundary") # Open a polygon shapefile with a single feature, representing the state of california. For example you could use the following dataset (but before loading the file in this script, remove all states except California): https://catalog.data.gov/dataset/usgs-small-scale-dataset-state-boundaries-of-the-united-states-200506-shapefile
CA <- spTransform(CA, CRS("+init=epsg:3310")) # If not already, put on CA Albers projection
CA@data$id = rownames(CA@data)
CA.points = fortify(CA, region="id")
CA.bound = join(CA.points, CA@data, by="id")
CA.bound$alb.x = CA.bound$long/1000
CA.bound$alb.y = CA.bound$lat/1000

## Subplot A: CWD map
p1a=
  ggplot(d.unique)+
  geom_tile(aes(x=alb.x*1000,y=alb.y*1000,fill = Defnorm,color=Defnorm))+
  scale_fill_gradientn(colours=viridis(256),limits=c(400,1000),breaks=c(400,500,600,700,800,900,1000),labels=c("< 400",500,600,700,800,900,"> 1000"),oob=squish,na.value='white')+
  scale_color_gradientn(colours=viridis(256),limits=c(400,1000),breaks=c(400,500,600,700,800,900,1000),labels=c("< 400",500,600,700,800,900,"> 1000"),oob=squish,na.value='white',guide='none')+
  labs(fill=expression('Mean annual\\nCWD (mm)'))+
  ggsn::scalebar(data=NULL, x.min=min(d.unique$alb.x*1000),x.max=max(d.unique$alb.x*1000-1000),
                 y.min=min(d.unique$alb.y*1000),y.max=max(d.unique$alb.y*1000), anchor=c(x=-380*1000,y=-550*1000),
                 dist = 200,  model = 'GRS80', location="bottomleft",st.size=3.5) +
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        legend.position=c(0.78,0.8),
        plot.margin=unit(c(.5,.5,.5,.5), "cm"),
        panel.border = element_rect(colour = "black", fill=NA, size=1))+
  geom_path(data=CA.bound,aes(alb.x*1000,alb.y*1000,group=group), color="black")+
  coord_fixed(xlim=c(-400*1000, 380*1000)) +
  theme(legend.title=element_text(size=9.5))

## Subplot b: basal area map
p1b=
  ggplot(d.unique)+
  geom_tile(aes(x=alb.x,y=alb.y,fill = live.bah,color=live.bah))+
  scale_fill_gradientn(colours=inferno(256),limits=c(0,60),oob=squish,na.value='white')+
  scale_color_gradientn(colours=inferno(256),limits=c(0,60),oob=squish,na.value='white',guide='none')+
  labs(fill=expression(paste('Live basal area (', m^2,ha^-1,')',sep="")))+
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        legend.position=c(0.78,0.8),
        plot.margin=unit(c(.5,.5,.5,.5), "cm"),
        panel.border = element_rect(colour = "black", fill=NA, size=1))+
  geom_path(data=CA.bound,aes(alb.x,alb.y,group=group), color="black")+
  coord_fixed(xlim=c(-400, 380)) +
  theme(legend.title=element_text(size=9.5))

## Merge subplots into a single plot
a1=ggplotGrob(p1a)#Create a Grob from fig 'a'
b1=ggplotGrob(p1b)#Create a Grob from the first plot in 'b'

##Save as tiff:
tiff(paste0("Figures/Fig1_",Sys.Date(),".tiff"),res=300,width=2800,height=2000)
F1=grid.arrange(a1,b1,nrow=1)
dev.off()





#-----------------------------------------------------#
####  11. Fig. 2- Prediction plot for all years  ######
#-----------------------------------------------------#

## Select an intermediate live.bah value (20 m2 ha-1) and plot predictions for all years side-by-side (with confidence intervals)

# Lists to hold plots--binomial (mortality probability) and Gaussian (mortality amount)
plot.mortbin.list <- list()
plot.morttpa.list <- list()

#For nonspatial models, all years
year.opts <- c("2009_nonspatial","2010_nonspatial","2011_nonspatial","2012_nonspatial","2013_nonspatial","2014_nonspatial","2015_nonspatial")

# For each year, plot predictions of the binomial and Gaussian components of the hurdle model
for(year in year.opts) {
  
  df.plot <- counterfactual.df.list[[as.character(year)]] # the data frame of predictor variables and predicted values for the year in question; "counterfactual" refers to the fact that we are making predictions for data points that do not exactly replicate the observed data points
  df.plot <- df.plot[df.plot$live.bah == 20,] # only where live.bah == 20 m2 ha-1
  
  
  # Find the extreme values of observed Defnorm where live.bah == 20 (+-3), and trim the prediction DF to only show predictions for Defnorm values that exist at this live.bah level (dropping the 1% upper and lower quantiles of observed Defnorm values at this live.bah value)
  bah.focal <- 20
  d.yr.bah <- d[(d$year==year) & (d$live.bah > (bah.focal - 3)) & (d$live.bah < (bah.focal + 3)),]
  deflim <- quantile(d.yr.bah$Defnorm,probs=c(0.01,0.99)) # the extremes of Defnorm at live.bah==20 (+-3)
  df.plot <- df.plot[(df.plot$Defnorm > deflim[1]) & (df.plot$Defnorm < deflim[2]),]
  
  p2.name <- paste("p2",year,sep=".") # p2.year will hold the binomial model predictions for the focal year
  p3.name <- paste("p3",year,sep=".") # p3.year will hold the Gaussian model predictions for the focal year
  
  # Plot predictions of binomial model
  plot.mortbin.list[[p2.name]] <- ggplot(df.plot, aes(x=Defnorm)) + 
    geom_ribbon(aes(ymin=bin.confint.low, ymax=bin.confint.high), fill="lightsalmon") + 
    geom_line(aes(y=bin.response.median),color="black") + xlab("") + ylab("") + 
    coord_cartesian(ylim=c(0, 1)) + theme_bw(16) + 
    theme(text = element_text(size=17), plot.margin = unit(c(.5,3.5,.5,.5), "mm"),panel.grid.minor = element_blank()) +
    xlim(400,1000) + 
    guides(alpha=FALSE,fill=FALSE,color=FALSE)
  
  # Plot predictions of Gaussian model
  plot.morttpa.list[[p3.name]] <- ggplot(df.plot, aes(x=Defnorm)) + geom_ribbon(aes(ymin=tph.confint.low*100, ymax=tph.confint.high*100), fill="lightsalmon") + geom_line(aes(y=tph.response.median*100),color="black") + xlab("") + ylab("") + 
    coord_cartesian(ylim=c(0, 500)) + theme_bw(16) + guides(alpha=FALSE,fill=FALSE,color=FALSE) + 
    theme(text = element_text(size=17), plot.margin = unit(c(.5,3.5,.5,.5), "mm"),panel.grid.minor = element_blank()) + xlim(400,1000)

  #add y-axis labels if it's the first plot in the row
  if(year == "2009_nonspatial"){
    plot.mortbin.list[[p2.name]]=plot.mortbin.list[[p2.name]]+
      labs(y=expression(atop('Predicted probability of','a mortality event')))+
      theme(axis.title.y = element_text(size=15))
    
    plot.morttpa.list[[p3.name]]=plot.morttpa.list[[p3.name]]+
      labs(y=expression(atop('Predicted number of','dead trees'~km^-2)))+
      theme(axis.title.y = element_text(size=15))
  }
  
}

## Plot climate departures fron normal (precip, temp, and CWD) from 2009 to 2015 (this does not use model predictions)

# Only show cells that were surveyed in all years
d$plot.id <- paste(d$pos.x,d$pos.y,sep=".") # create a unique "plot id" based on its coordinates
d.fullfits <- d[d$year %in% as.character(2009:2015),]
d.fullfits$year <- as.numeric(d.fullfits$year)
plot.yearcount <- aggregate(d.fullfits[,c("plot.id")],by=list(d.fullfits$plot.id),FUN=length) # get number of years for each plot
names(plot.yearcount) <- c("plot.id","nyears")
plots.allyears <- plot.yearcount[plot.yearcount$nyears == 7,]$plot.id # only use plots surveyed in all 7 years
d.preclim <- d.fullfits[d.fullfits$plot.id %in% plots.allyears,]
d.preclim <- d.preclim[d.preclim$year %in% 2009:2015,]

# Prepare a data frame representing climate variation across years, including bands encompassing the 25th-to-75th percentiles of the range across the state in each year
# Pz0, Tz0, and Dz0 are the the z-values (number of standard deviations from long-term mean) of precipitation, temperature, and deficit, respectively, in each year
d.clim=ddply(d.preclim,.(year),summarize,
             Pz=mean(Pz0),PzSE=sd(Pz0)/sqrt(length(Pz0)),Pz05=quantile(Pz0,c(0.25))[[1]],Pz95=quantile(Pz0,c(0.75))[[1]],
             Tz=mean(Tz0),TzSE=sd(Tz0)/sqrt(length(Tz0)),Tz05=quantile(Tz0,c(0.25))[[1]],Tz95=quantile(Tz0,c(0.75))[[1]],
             Dz=mean(Defz0),DzSE=sd(Defz0)/sqrt(length(Defz0)),
             Dz05=quantile(Defz0,c(0.25))[[1]],Dz95=quantile(Defz0,c(0.75))[[1]] )

# Transform data frame into a ggplot-friendly form
d.clim.long.mean=melt(d.clim,id.vars="year",measure.vars=c("Tz","Dz","Pz"))
d.clim.long.pct=melt(d.clim,id.vars="year",measure.vars=c("Tz05","Dz05","Pz05"),value.name="P05",variable.name="climate_var")
d.clim.long.pct$P05=melt(d.clim,id.vars="year",measure.vars=c("Tz05","Dz05","Pz05"))[,"value"]
d.clim.long.pct$P95=melt(d.clim,id.vars="year",measure.vars=c("Tz95","Dz95","Pz95"))[,"value"]
d.clim.long.se=cbind(d.clim.long.mean,melt(d.clim,id.vars="year",measure.vars=c("PzSE","TzSE","DzSE")))
names(d.clim.long.se)[6]="se"
names(d.clim.long.pct)[2] <- "climate_var"
d.clim$year <- as.numeric(d.clim$year)

# Plot climate variation over time
F2a=
  ggplot(d.clim)+
  geom_line(data=d.clim.long.mean, aes(x=year,y=value,color=variable))+
  geom_ribbon(data=d.clim.long.pct,aes(x=year,ymax=P95,ymin=P05,fill=climate_var),alpha=0.2)+ #previously had guide=F
  scale_fill_discrete(guide=FALSE)+
  theme_bw()+
  theme(legend.position=c(0.15, 0.75),legend.background = element_rect(fill="transparent"),
        axis.text.x = element_text(size=16),axis.title.y = element_text(size=13),panel.grid.minor = element_blank(),plot.margin = unit(c(.5,5,0,10), "mm"))+
  scale_colour_discrete(name = "Climate variable",labels=c("Temperature","Climatic water deficit", "Precipitation"))+
  scale_x_continuous(breaks=c(2009:2015))+
  geom_hline(yintercept=0,lty=2)+
  geom_point(data=d.clim.long.mean,aes(x=year,y=value)) +
  labs(y="\\nAnnual climate anomaly",x="") #+


## Merge all plots into one figure
b1=ggplotGrob(plot.mortbin.list[[1]])#Create a Grob from the first plot in 'b'
c1=ggplotGrob(plot.morttpa.list[[1]])#Create a Grob from the first plot in 'c'

F2b=arrangeGrob(b1,
                plot.mortbin.list[[2]],plot.mortbin.list[[3]],
                plot.mortbin.list[[4]],plot.mortbin.list[[5]],plot.mortbin.list[[6]],
                plot.mortbin.list[[7]],
                c1,plot.morttpa.list[[2]],
                plot.morttpa.list[[3]],
                plot.morttpa.list[[4]],plot.morttpa.list[[5]],plot.morttpa.list[[6]],
                plot.morttpa.list[[7]],
                nrow=2,widths=c(1.1,1,1,1,1,1,1)
)

# Save as PNG
#Cairo(file=paste0("Figures/Fig2_",Sys.Date(),".png"),width=3300,height=1750,ppi=200,res=200,dpi=200)
F2=grid.arrange(F2a,F2b, nrow=3,heights=c(0.65,1,0.07))
#dev.off()




#-----------------------------------------------------#
####  12. Fig. 3- Scatterplot and map predictions  ####
#-----------------------------------------------------#

d.2015=d[d$year=="2015_nonspatial" ,] #Subset to 2015 data

## Predict median mortality amount for each 2015 data point

# Median mortality probability. Make sure bin.param.sim.mat is from 2015 (it is if the loop in Section 9 was run in its entirety)
d.2015$mort.bin.pred <- median(bin.param.sim.mat[,"(Intercept)"]) +
  median(bin.param.sim.mat[,"Defnorm_std"]) * (d.2015[,"Defnorm_std"]) +
  median(bin.param.sim.mat[,"live.bah_std"]) * d.2015[,"live.bah_std"] +
  median(bin.param.sim.mat[,"I(live.bah_std^2)"]) * d.2015[,"live.bah_std"]^2 +
  #median(bin.param.sim.mat[,"I(Defnorm_std^2)"]) * d.2015[,"Defnorm_std"]^2 +
  median(bin.param.sim.mat[,"Defz0"]) * d.2015[,"Defz0"] +
  median(bin.param.sim.mat[,"Defnorm_std:live.bah_std"]) * d.2015[,"Defnorm_std"] * d.2015[,"live.bah_std"]
d.2015$mort.bin.pred <- invlogit(d.2015$mort.bin.pred) # transform back to response scale

# Median mortality amount. Make sure tph.param.sim is from 2015 
d.2015$mort.tph.pred <- median(tph.param.sim.mat[,"(Intercept)"]) +
  median(tph.param.sim.mat[,"Defnorm_std"]) * d.2015[,"Defnorm_std"] +
  median(tph.param.sim.mat[,"live.bah_std"]) * d.2015[,"live.bah_std"] +
  median(tph.param.sim.mat[,"I(Defnorm_std^2)"]) * d.2015[,"Defnorm_std"]^2 +
  median(tph.param.sim.mat[,"I(live.bah_std^2)"]) * d.2015[,"live.bah_std"]^2 +
  median(tph.param.sim.mat[,"Defz0"]) * d.2015[,"Defz0"] +
  median(tph.param.sim.mat[,"Defnorm_std:live.bah_std"]) * d.2015[,"Defnorm_std"] * d.2015[,"live.bah_std"]
d.2015$mort.tph.pred <- exp(d.2015$mort.tph.pred) # convert back to response scale

# For plotting, the response variables are put on the log(tph+1) scale, so pixels with no mortality still show up as 0. We also truncate the data where mort.tph>~400 (log(mort.tph)>6) for easier visualization. We multiply TPH by 100 to get trees per km2
d.2015$mort.logadj.tpkm.pred=log(ceiling(d.2015$mort.tph.pred*100)+1) 
d.2015$mort.logadj.tpkm=log(ceiling(d.2015$mort.tph*100)+1)
d.2015$mort.logadj.tpkm.pred[d.2015$mort.logadj.tpkm.pred>6]=6 #Truncate upper end
d.2015$mort.logadj.tpkm[d.2015$mort.logadj.tpkm>6]=6 #Truncate upper end

# For mapping, set mortality probability threshold below which a cell is assigned no mortality such that number of cells with no mortality equal observed number of cells with no mortality in 2015
d.2015.maps=d.2015[order(d.2015$mort.bin.pred),]
d.2015.maps[c(1:length(which(d.2015$mort.bin==0))),"mort.logadj.tpkm.pred"]=0

# For scatterplots, assign presence/absence of mortality using a binomial process on every cell
set.seed(2) #So the figure is the same every time.
mort.index= rbinom(n=nrow(d.2015),size=1,prob=d.2015$mort.bin.pred) #Binomial draw
d.2015$mort.logadj.tpkm.pred[which(mort.index==0)]=0 #Set the cells that didn't have any mortality to 0

# Set up color scheme for Figure
brks=seq(0.5, 6, by=0.5)
nb <- length(brks)-1 # nb= number of breaks. Minus one because of syntax, makes everything above 400 the same color
palette.breakpoints=seq(0,1,length=nb) # How many colors to use in scale
palette.cols=seq_gradient_pal(low='#FFFF00',high='#8B1919')(palette.breakpoints) #Colors to plot over the continuous scale. #FFFF00=yellow, #8B1919=firebrick4
all.cols= c("forest green",palette.cols,'#8B1919' ) #Add the end-point colors

# Compute the FDCI curve for panel A
FDCI80=d[between(d$Defquant,0.8,0.81),]
FDCI50=d[between(d$Defquant,0.5,0.51),]

## Make scatterplot panels for Figure 3

# Predicted mortality
p1.d.pred=d.2015[sample(1:nrow(d.2015),size=nrow(d.2015),replace=FALSE),] # randomize plotting order of points
p1= 
  ggplot(p1.d.pred) + 
  geom_point(aes(x=Defnorm, y=live.bah, color=mort.logadj.tpkm.pred), stroke=0, size=1.0)+
  geom_smooth(data=FDCI80,aes(x=Defnorm,y=live.bah),method='loess',color='black',alpha=0) + 
  geom_smooth(data=FDCI50,aes(x=Defnorm,y=live.bah),method='loess',color='black',alpha=0,lty=2) + 
  scale_color_gradientn(colours=all.cols,breaks=c(0.1,0.5,2,4,6),na.value="white",
                        guide='none')+
  xlim(c(min(d.2015$Defnorm),max(d.2015$Defnorm)))+
  ylim(c(min(d.2015$live.bah),max(d.2015$live.bah)))+
  labs(title = 'Predicted mortality (2015)', y=expression(paste('Live basal area (', m^2,ha^-1,')',sep="")),x="Mean annual CWD (mm)") +
  theme_bw()

# Observed mortality
p2.d.pred=d.2015[sample(1:nrow(d.2015),size=nrow(d.2015),replace=FALSE),] # randomize plotting order of points
p2=
  ggplot(p2.d.pred) + 
  geom_point(aes(x=Defnorm, y=live.bah, color=mort.logadj.tpkm), stroke=0, size=1.0)+
  scale_color_gradientn(colours=all.cols,breaks=c(0.01,0.8,2,4,6),na.value="white",
                        labels=c("0","1",paste(round(exp(2),0)),
                                 round(exp(4),0),paste(">",round(exp(6),-1))))+
  xlim(c(min(d.2015$Defnorm),max(d.2015$Defnorm)))+
  ylim(c(min(d.2015$live.bah),max(d.2015$live.bah)))+
  labs(title = 'Observed mortality (2015)', 
       y=expression(paste('Live basal area (', m^2,ha^-1,')',sep="")),
       x="Mean annual CWD (mm)",
       color=expression('Dead trees km'^'-2')) +
  theme_bw()+ theme(legend.position=c(0.8,0.7),legend.background = element_rect(fill="transparent"))


## Make map panels for Figure 3

# Predicted mortality
p3.d.pred=am_rasterize(d.2015.maps,layer="mort.logadj.tpkm.pred") #Rasterize the predicted mortality layer. Warning message just means there are empty (non-flown) cells
p4.d.obs=am_rasterize(d.2015.maps,layer="mort.logadj.tpkm") #Rasterie the observed mortality layer. Warning message just means there are empty (non-flown) cells
p3=
  ggplot(d.2015.maps)+
  geom_tile(aes(x=alb.x*1000,y=alb.y*1000,fill = mort.logadj.tpkm.pred,color=mort.logadj.tpkm.pred))+
  scale_fill_gradientn(colours=all.cols,breaks=c(0.1,0.5,2,4,6),na.value="white",guide='none')+
  scale_color_gradientn(colours=all.cols,breaks=c(0.1,0.5,2,4,6),na.value="white",guide='none')+
  ggsn::scalebar(data=NULL, x.min=min(d.unique$alb.x*1000),x.max=max(d.unique$alb.x*1000-1000),
                 y.min=min(d.unique$alb.y*1000),y.max=max(d.unique$alb.y*1000), anchor=c(x=-380*1000,y=-550*1000),
                 dist = 200,  model = 'GRS80', location="bottomleft",st.size=3.5) +
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.border = element_rect(colour = "black", fill=NA, size=1),
        panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        legend.position=c(0.9,0.7),
        plot.margin=unit(c(0.5,0,0,0), "cm"))+
  geom_path(data=CA.bound,aes(alb.x*1000,alb.y*1000,group=group), color="black")+
  coord_fixed(xlim=c(-400*1000, 380*1000))

# Observed mortality
p4=
  ggplot(d.2015.maps)+
  geom_tile(aes(x=alb.x,y=alb.y,fill = mort.logadj.tpkm,color=mort.logadj.tpkm))+
  scale_fill_gradientn(colours=all.cols,breaks=c(0.1,0.5,2,4,6),na.value="white",guide='none')+
  scale_color_gradientn(colours=all.cols,breaks=c(0.1,0.5,2,4,6),na.value="white",guide='none')+
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.border = element_rect(colour = "black", fill=NA, size=1),
        panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        legend.position=c(0.9,0.7),
        plot.margin=unit(c(0.5,0,0,0), "cm"))+
  geom_path(data=CA.bound,aes(alb.x,alb.y,group=group), color="black") +
  coord_fixed(xlim=c(-400, 380))

# Create grobs from the four panels to merge into final plot
a1=ggplotGrob(p1)
b1=ggplotGrob(p2)
c1=ggplotGrob(p3)
d1=ggplotGrob(p4)


# Save as tiff
tiff(paste0("Figures/Fig3_",Sys.Date(),".tiff"),res=300,width=2600,height=2600)
F3=grid.arrange(a1,b1,c1,d1, nrow=2)
dev.off()





#-----------------------------------------------------#
####  13. Fig. 4- FDCI  ###############################
#-----------------------------------------------------#

#Subset to 2015 mortality pixels
d.2015 <- d[d$year=="2015_nonspatial",]
d.2015.mort = d.2015[d.2015$mort.bin==1 ,]

# FDCI (referred to as "Defquant" in the data frame) was calculated in the aerial mortality data fram generation script (Young_et_al_Data_preparation.R). The def_quant_calc function determines what quantile the CWD value for a given pixel falls into, within the full range of possible CWD values for pixels that have similar BA values to the pixel in question (within 2.5 m2/ha)

# To justify FCDI
summary(lm(log(mort.tph)~(Defquant^2), data=d.2015.mort))
BIC(lm(log(mort.tph)~1, data=d.2015.mort))- #Null model; dBIC = 784 points worse than the best model of FDCI below.
BIC(lm(log(mort.tph)~Defnorm*live.bah + I(live.bah^2) + I(Defnorm^2) + Defz0, data=d.2015.mort)) - #Best model analyzed in the rest of the paper for mort.tph; dBIC = 340 points better than the best model of FDCI below.
BIC(lm(log(mort.tph)~I(Defquant^2), data=d.2015.mort)) #Best model of FDCI

# Panel A: mortality amount vs. FDCI scatterplot
p4a=
  ggplot(d.2015.mort,aes(x=Defquant,y=(mort.tph*100),col=Defquant)) + 
  geom_point(size=1) + 
  geom_smooth(method='lm',formula = y ~ poly(x, 2),color='black') + 
  labs(x='FDCI',y=expression(paste('Dead trees ', km^-2,sep=""))) +
  theme_bw()+
  theme(plot.margin=unit(c(1,0,0,0), "cm"),panel.grid.minor = element_blank()) +
  scale_color_gradient2(low='forest green', mid='orange',high='darkred',midpoint=0.5,na.value='white',guide='none')+
  scale_y_log10()

# Panel B: map of FDCI
p4c.d=am_rasterize(d,layer="Defquant") #Rasterize the mortality data frame. Warning message just means there are empty (non-flown) cells

#Set up the CA state boundary

# Generate California boundary
CA = readOGR(dsn="./GIS", layer="CA_Boundary") # Open a polygon shapefile with a single feature, representing the state of california
CA <- spTransform(CA, CRS("+init=epsg:3310")) #Put on CA Albers coordinate system
CA@data$id = rownames(CA@data)
CA.points = fortify(CA, region="id")
CA.bound = join(CA.points, CA@data, by="id")
CA.bound$alb.x = CA.bound$long/1000
CA.bound$alb.y = CA.bound$lat/1000

p4c=
  ggplot(d.unique)+
  geom_tile(aes(x=alb.x*1000,y=alb.y*1000,fill = Defquant,color=Defquant))+
  scale_fill_gradient2(low='forest green', mid='gold',high='darkred',midpoint=0.5,na.value='white')+
  scale_color_gradient2(low='forest green', mid='gold',high='darkred',midpoint=0.5,na.value='white',guide='none')+
  labs(fill=expression('FDCI'))+
  ggsn::scalebar(data=NULL, x.min=min(d.unique$alb.x*1000),x.max=max(d.unique$alb.x*1000-1000),
                 y.min=min(d.unique$alb.y*1000),y.max=max(d.unique$alb.y*1000), anchor=c(x=-380*1000,y=-550*1000),
                 dist = 200,  model = 'GRS80', location="bottomleft",st.size=3.5) +
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.border = element_rect(colour = "black", fill=NA, size=1),
        panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
        plot.background=element_blank(),
        legend.position=c(0.78,0.78),
        plot.margin=unit(c(1,.6,.6,0), "cm"))+
  geom_path(data=CA.bound,aes(alb.x*1000,alb.y*1000,group=group), color="black")+
  coord_fixed(xlim=c(-400*1000, 380*1000))


# Convert panels to grobs
p4a3=ggplotGrob(p4a)
p4c3=ggplotGrob(p4c)


## Merge into one plot and save as tiff
tiff(paste0("Figures/Fig4_",Sys.Date(),".tiff"),res=300,width=2400,height=1800)
grid.arrange(p4a3,p4c3,ncol=2,widths=c(12,14))
dev.off()




#-----------------------------------------------------#
####  14. Fig S1: observed mortalty in each year  #####
#-----------------------------------------------------#

# Log-transform observed mortality prior to assigning it a color scale
d$mort.logadj.tpkm=log(ceiling(d$mort.tph*100)+1)
d$mort.logadj.tpkm[d$mort.logadj.tpkm>6]=6 #Truncate upper end

# Set up color scheme for Figure
brks=seq(0.5, 6, by=0.5)
nb <- length(brks)-1 #nb= number of breaks. Minus one because of syntax, makes everything above 400 the same color
palette.breakpoints=seq(0,1,length=nb) # How many colors to use in scale
palette.cols=seq_gradient_pal(low='#FFFF00',high='#8B1919')(palette.breakpoints) #C olors to plot over the continuous scale. #FFFF00=yellow, #8B1919=firebrick4
all.cols= c("forest green",palette.cols,'#8B1919' ) # Add the end-point colors

# For each year, plot a map of observed mortality
map.panel <- list() # for holding the plot for all years
for(year in 2009:2015) {
  
  d.year <- d[d$year == year,]

  
    map.panel[[as.character(year)]] <- ggplot(d.year,aes(x=alb.x*1000,y=alb.y*1000,color=mort.logadj.tpkm))+
      geom_tile(aes(color=mort.logadj.tpkm,fill=mort.logadj.tpkm))+
      scale_fill_gradientn(colours=all.cols,breaks=c(0.01,0.8,2,4,6),na.value="white",guide=FALSE)+
      scale_color_gradientn(colours=all.cols,breaks=c(0.01,0.8,2,4,6),na.value="white", guide=FALSE)+
      theme_bw(15) +
      theme(axis.line=element_blank(),
            axis.text.x=element_blank(), axis.text.y=element_blank(),
            axis.ticks=element_blank(),
            axis.title.x=element_blank(), axis.title.y=element_blank(),
            panel.background=element_blank(),
            panel.border = element_rect(colour = "black", fill=NA, size=1),
            panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
            plot.background=element_blank(),
            legend.position=c(0.9,0.7),
            plot.margin=unit(c(0,0,0,0), "cm"))+
      geom_path(data=CA.bound,aes(alb.x*1000,alb.y*1000,group=group), color="black") +
      coord_fixed(xlim=c(-400*1000, 380*1000)) +
      ggtitle(year)
    
    
    if(year==2009) {
      
      map.panel[[as.character(year)]] <- map.panel[[as.character(year)]] + 
          ggsn::scalebar(data=NULL, x.min=min(d.unique$alb.x*1000),x.max=max(d.unique$alb.x*1000-1000),
                         y.min=min(d.unique$alb.y*1000),y.max=max(d.unique$alb.y*1000), anchor=c(x=-380*1000,y=-550*1000),
                         dist = 200,  model = 'GRS80', location="bottomleft",st.size=5)

    }
    
    
}

## Procude a legend to plot as separate panel

# Fake plot with a legend
fake <- ggplot(d.year,aes(x=alb.x,y=alb.y,color=mort.logadj.tpkm))+
  geom_tile(aes(fill=mort.logadj.tpkm))+
  scale_fill_gradientn(colours=all.cols,breaks=c(0.01,0.8,2,4,6),na.value="white",guide=FALSE)+
  scale_color_gradientn(colours=all.cols,breaks=c(0.01,0.8,2,4,6),na.value="white",labels=c("0","1",paste(round(exp(2),0)), round(exp(4),0),paste(">",round(exp(6),-1)))) +
  labs(color=expression('Dead trees km'^'-2')) +
  theme_bw(20)

# Function to extract legend
g_legend<-function(a.gplot){ 
  tmp <- ggplot_gtable(ggplot_build(a.gplot)) 
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box") 
  legend <- tmp$grobs[[leg]] 
  return(legend)} 

# Extract legend
legend <- g_legend(fake)

# Plot final multi-panel plot
Cairo(file=paste0("Figures/FigS1_",Sys.Date(),".tiff"),typ="png",width=1000,height=1000)
p=grid.arrange(map.panel[[1]],map.panel[[2]],map.panel[[3]],map.panel[[4]],map.panel[[5]],map.panel[[6]],map.panel[[7]],legend,ncol=4)
dev.off()





#---------------------------------------------------------------------------------#
####  15. Fig. S2- Predicted mortality along variation in basal area for 2015  ####
#---------------------------------------------------------------------------------#

# Select two CWD values (600 and 900 mm) and plot predictions along variation in basal area for all years side-by-side (with confidence and prediction intervals)
plot.mortbin.list <- list() # for holding plots for mortality probability each year (only will hold one plot since only plotting for 2015)
plot.morttpa.list <- list() # for holding plots for mortality amount each year (only will hold one plot since only plotting for 2015
def.values.plot <- c(600,900) # for which deficit values should we make the plot?

# For all years of interest (we are only plotting for 2015)
for(year in "2015_nonspatial") {
  
  df.plot <- counterfactual.df.list[[as.character(year)]] # retrieve the data frame of predicted values for this year
  df.plot <- df.plot[df.plot$Defnorm %in% def.values.plot,] # thin the predictions data frame to only the focal CWD values
  df.plot$Defnorm <- as.character(df.plot$Defnorm)

  # Find the extreme values of observed live.bah where Defnorm == 600 or 900 (+-50), and trim the prediction DF to only show predictions for live.bah values that exist at this Defnorm level (dropping the 1% upper and lower quantiles of observed Defnorm values at this live.bah value)
  for(def in unique(df.plot$Defnorm)) {
    
    Defnorm.focal <- as.numeric(def)
      
    d.yr.bah <- d[(d$year==year) & (d$Defnorm > (Defnorm.focal - 50)) & (d$Defnorm < (Defnorm.focal + 50)),] # accept
    live.bah.lim <- quantile(d.yr.bah$live.bah,probs=c(0.01,0.99)) # Extreme upper and lower values of live.bah at this level of Defnormm, excluding the upper and lower 1st percentiles
    df.plot <- df.plot[((df.plot$Defnorm == def) & (df.plot$live.bah > live.bah.lim[1]) & (df.plot$live.bah < live.bah.lim[2])) | (df.plot$Defnorm != def),]
  
  }
  
  p2.name <- paste("p2",year,sep=".") # p2.year will hold the binomial model predictions for the focal year
  p3.name <- paste("p3",year,sep=".") # p2.year will hold the Gaussian model predictions for the focal year
  
  # Plot predictions of binomial model
  plot.mortbin.list[[p2.name]] <- ggplot(df.plot, aes(x=live.bah,col=Defnorm)) + 
    geom_ribbon(aes(ymin=bin.confint.low, ymax=bin.confint.high,fill=Defnorm),alpha=0.5,color=NA) + 
    geom_line(aes(y=bin.response.median,color=Defnorm)) + xlab("") + ylab("") + 
    coord_cartesian(ylim=c(0, 1)) + theme_bw(16) + 
    theme(plot.margin = unit(c(3,3,.5,.5), "mm")) + xlim(0,50) + 
    guides(alpha=FALSE,fill=FALSE,color=FALSE) +
    scale_fill_manual(values=c("forestgreen","chocolate")) +
    scale_color_manual(values=c("forestgreen","chocolate"))
  
  # Plot predictions of Gaussian model
  plot.morttpa.list[[p3.name]] <- ggplot(df.plot, aes(x=live.bah,col=Defnorm)) + 
    #geom_ribbon(aes(ymin=tph.predint.low*100, ymax=tph.predint.high*100), fill="lightsalmon") + 
    geom_ribbon(aes(ymin=tph.confint.low*100, ymax=tph.confint.high*100,fill=Defnorm),alpha=0.5,color=NA) + 
    geom_line(aes(y=tph.response.median*100,color=Defnorm)) + xlab("") + ylab("") + 
    coord_cartesian(ylim=c(0, 500)) + theme_bw(16) + guides(alpha=FALSE) + 
    theme(plot.margin = unit(c(.5,3,.5,.5), "mm"),legend.position=c(0.78,0.7)) + xlim(0,50) +
    labs(fill=expression('Mean annual CWD (mm)'),color=expression('Mean annual CWD (mm)')) +
    scale_fill_manual(values=c("forestgreen","chocolate")) +
    scale_color_manual(values=c("forestgreen","chocolate"))
    
  
  #add y-axis labels if it's the first plot in the row (which it is since it's the only plot in the row)
  if(year %in% c(2015,"2015_nonspatial")){
    plot.mortbin.list[[p2.name]]=plot.mortbin.list[[p2.name]]+
      labs(y=expression(atop('Predicted probability of','a mortality event')))+
      theme(axis.title.y = element_text(size=13))
    
    plot.morttpa.list[[p3.name]]=plot.morttpa.list[[p3.name]]+
      labs(y=expression(atop('Predicted number of','Dead trees'~km^-2)))+
      theme(axis.title.y = element_text(size=13))
  }
  
}

# Compile the panels into one large figure
b1=ggplotGrob(plot.mortbin.list[[1]])#Create a Grob from the first plot in 'b'
c1=ggplotGrob(plot.morttpa.list[[1]])#Create a Grob from the first plot in 'c'
F2b=arrangeGrob(b1,c1,nrow=2,widths=c(1.1))

# Save plot as TIFF
#tiff(paste0("Figures/FigS3_",Sys.Date(),".tiff"),res=300,width=2400,height=2000)
F2=grid.arrange(F2b,nrow=1,heights=c(1))
#dev.off()




#-------------------------------------------------------#
####  Fig. S3- Scatterplot of mort.tph vs. live.tph  ####
#-------------------------------------------------------#

# Select only 2015 mortality data
d.2015=d[d$year==2015 & d$mort.bin==1,]
d.2015 <- d.2015[sample(1:nrow(d.2015),size=nrow(d.2015),replace=FALSE),] # randomize plotting order of points

# Make plot
p1=
  ggplot(d.2015,aes(x=live.tph*100,y=mort.tph*100,color=Defnorm))+
    geom_point(size=2) +
    theme_bw() +
    scale_color_gradientn(colours=viridis(256),limits=c(400,1000),breaks=c(400,600,800,1000),labels=c("< 400",600,800,"> 1000"),oob=squish,na.value='white') +
    labs(color=expression('Mean annual\\nCWD (mm)'),y=expression(paste('Dead trees ',km^-2,sep="")),x=expression(paste('Live trees ',km^-2,sep=""))) +
    scale_y_log10() +
    stat_smooth(method='lm',formula=y~x,color="black", level=0.95)

# Save figure to TIFF
tiff(paste0("Figures/FigS4_",Sys.Date(),".tiff"),res=300,width=2400,height=2000)
p1
dev.off()